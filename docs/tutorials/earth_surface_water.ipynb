{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vvangfaye/torchgeo/blob/main/docs/tutorials/earth_surface_water.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLbpQN76N6h9"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "# Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAR190Aszv8r"
      },
      "source": [
        "# Earth Water Surface\n",
        "\n",
        "_Written by: Mauricio Cordeiro_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lZeGJNTz1y5"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXG-oXGUz39X"
      },
      "source": [
        "The objective of this tutorial is to go through the Earth Water Surface dataset and cover the following topics:<br>\n",
        "\n",
        "* Creating RasterDatasets, DataLoaders and Samplers for images and masks;\n",
        "* Intersection Dataset;\n",
        "* Normalizing the data;\n",
        "* Creating spectral indices;\n",
        "* Creating the segmentation model (DeepLabV3);\n",
        "* Loss function and metrics; and\n",
        "* Training loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTt_Ysyl4El5"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U9kWwoL4GqT"
      },
      "source": [
        "For the environment, we will install the torchgeo and scikit-learn packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A14-syGAFahE",
        "outputId": "35e0e268-b13c-4a7b-d289-ec3158f3aa5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchgeo\n",
            "  Downloading torchgeo-0.6.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.8.1)\n",
            "Collecting fiona>=1.8.21 (from torchgeo)\n",
            "  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia>=0.7.3 (from torchgeo)\n",
            "  Downloading kornia-0.8.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lightly!=1.4.26,>=1.4.5 (from torchgeo)\n",
            "  Downloading lightly-1.5.19-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting lightning!=2.3.*,>=2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.2.2)\n",
            "Requirement already satisfied: pillow>=8.4 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (11.1.0)\n",
            "Requirement already satisfied: pyproj>=3.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.7.1)\n",
            "Collecting rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3 (from torchgeo)\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting rtree>=1 (from torchgeo)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting segmentation-models-pytorch>=0.2 (from torchgeo)\n",
            "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.0.7)\n",
            "Requirement already satisfied: timm>=0.4.12 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.6.0+cu124)\n",
            "Collecting torchmetrics>=0.10 (from torchgeo)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchvision>=0.14 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.21.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo) (2025.1.31)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.21->torchgeo) (8.1.8)\n",
            "Collecting click-plugins>=1.0 (from fiona>=1.8.21->torchgeo)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona>=1.8.21->torchgeo)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting kornia_rs>=0.1.0 (from kornia>=0.7.3->torchgeo)\n",
            "  Downloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia>=0.7.3->torchgeo) (24.2)\n",
            "Collecting hydra-core>=1.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lightly_utils~=0.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.32.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (4.67.1)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.10.6)\n",
            "Collecting pytorch_lightning>=1.0.4 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.3.0)\n",
            "Collecting aenum>=3.1.11 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (4.12.2)\n",
            "Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading jsonargparse-4.37.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting omegaconf<3.0,>=2.2.3 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (13.9.4)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.45.2 (from lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5->torchgeo) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.3->torchgeo) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.3->torchgeo) (2025.1)\n",
            "Collecting affine (from rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3->torchgeo)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch>=0.2->torchgeo)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.2->torchgeo) (0.29.3)\n",
            "Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch>=0.2->torchgeo)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.4.12->torchgeo) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->torchgeo)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->torchgeo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->torchgeo) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (3.11.14)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo)\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (75.1.0)\n",
            "Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch>=0.2->torchgeo)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (5.29.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->torchgeo) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (0.1.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->torchgeo) (6.5.2)\n",
            "Downloading torchgeo-0.6.2-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly-1.5.19-py3-none-any.whl (850 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.8/850.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.37.0-py3-none-any.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.3/216.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, antlr4-python3-runtime, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=2e4f357a6e51c6e066c49441b4eb7cae7da7f8a5fead31bd1a6adf1af2cd6b76\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/6f/9b/231a832f811ab6ebb1b32455b177ffc6b8b1cd8de19de70c09\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=44125b6f03c7cb010b14699e0890b02a7397814ba97388cf93fda8c3f46934eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=6b90dd15f1c85d40a81a9229327f899cf7b486cbd90709e994e01291badc218d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built efficientnet-pytorch antlr4-python3-runtime pretrainedmodels\n",
            "Installing collected packages: antlr4-python3-runtime, aenum, typeshed-client, tensorboardX, rtree, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, lightly_utils, kornia_rs, jsonargparse, cligj, click-plugins, affine, rasterio, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, fiona, nvidia-cusolver-cu12, torchmetrics, kornia, efficientnet-pytorch, bitsandbytes, pytorch_lightning, pretrainedmodels, segmentation-models-pytorch, lightning, lightly, torchgeo\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n"
          ]
        }
      ],
      "source": [
        "%pip install torchgeo scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIBW1KULN6iB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1YPyqM4N6iB"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from collections.abc import Callable, Iterable\n",
        "from pathlib import Path\n",
        "\n",
        "import kornia.augmentation as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import rasterio as rio\n",
        "import torch\n",
        "from sklearn.metrics import jaccard_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchgeo.datasets import RasterDataset, stack_samples, unbind_samples, utils\n",
        "from torchgeo.samplers import RandomGeoSampler, Units\n",
        "from torchgeo.transforms import indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73l1wc-fFWuU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7mCdoq00Yo"
      },
      "source": [
        "The dataset we will use is the Earth Surface Water dataset [1] (licensed under Creative Commons Attribution 4.0 International Public License), which has patches from different parts of the world (Figure below) and its corresponding water masks. The dataset uses optical imagery from Sentinel-2 satellite with 10m of spatial resolution.\n",
        "\n",
        "![Image1](https://raw.githubusercontent.com/xinluo2018/WatNet/main/figures/dataset.png)\n",
        "\n",
        "[1] Xin Luo. (2021). Earth Surface Water Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.5205674\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wCou5sBN6iB"
      },
      "outputs": [],
      "source": [
        "# Download and extract dataset to a temp folder\n",
        "tmp_path = Path(tempfile.gettempdir()) / 'surface_water/'\n",
        "utils.download_and_extract_archive(\n",
        "    'https://hf.co/datasets/cordmaur/earth_surface_water/resolve/main/earth_surface_water.zip',\n",
        "    tmp_path,\n",
        ")\n",
        "\n",
        "# Set the root to the extracted folder\n",
        "root = tmp_path / 'dset-s2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abnT63f1GOh8"
      },
      "source": [
        "## Creating the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ke5sQ1_4nEq"
      },
      "source": [
        "Now that we have the original dataset already uncompressed in Colab’s environment, we can prepare it to be loaded into a neural network. For that, we will create an instance of the RasterDataset class, provided by TorchGeo, and point to the specific directory, using the following commands. The `scale` function will apply the `1e-4`  scale necessary to get the Sentinel-2 values in reflectance. Once the datasets are created, we can combine images with masks (labels) using the `&` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXiPVrjXGSes"
      },
      "outputs": [],
      "source": [
        "def scale(item: dict):\n",
        "    item['image'] = item['image'] / 10000\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWApUYVYZ1Lr"
      },
      "outputs": [],
      "source": [
        "train_imgs = RasterDataset(\n",
        "    paths=(root / 'tra_scene').as_posix(), crs='epsg:3395', res=10, transforms=scale\n",
        ")\n",
        "train_msks = RasterDataset(\n",
        "    paths=(root / 'tra_truth').as_posix(), crs='epsg:3395', res=10\n",
        ")\n",
        "\n",
        "valid_imgs = RasterDataset(\n",
        "    paths=(root / 'val_scene').as_posix(), crs='epsg:3395', res=10, transforms=scale\n",
        ")\n",
        "valid_msks = RasterDataset(\n",
        "    paths=(root / 'val_truth').as_posix(), crs='epsg:3395', res=10\n",
        ")\n",
        "\n",
        "# IMPORTANT\n",
        "train_msks.is_image = False\n",
        "valid_msks.is_image = False\n",
        "\n",
        "train_dset = train_imgs & train_msks\n",
        "valid_dset = valid_imgs & valid_msks\n",
        "\n",
        "# Create the samplers\n",
        "\n",
        "train_sampler = RandomGeoSampler(train_imgs, size=512, length=130, units=Units.PIXELS)\n",
        "valid_sampler = RandomGeoSampler(valid_imgs, size=512, length=64, units=Units.PIXELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaTZ03eJ5FGa"
      },
      "source": [
        "Note that we are specifying the CRS (Coordinate Reference System) to EPSG:3395. TorchGeo requires that all the images are loaded in the same CRS. However, the patches in the dataset are in different UTM projections and the default behavior of TorchGeo is to use the first CRS found as its default. In this case, we have to inform a CRS that is able to cope with these different regions around the globe. To minimize the deformations due to the huge differences in latitude (I can create a history specific for this purpose) within the patches, I have selected World Mercator as the main CRS for the project. Figure 3 shows the world projected in World Mercator CRS.\n",
        "\n",
        "\n",
        "![Image2](https://miro.medium.com/max/4800/1*sUdRKEfIAbm79jpB3bCShQ.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqNOU7WaOJ2t"
      },
      "source": [
        "### Understanding the sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RFrF3bTOSJn"
      },
      "source": [
        "To create training patches that can be fed into a neural network from our dataset, we need to select samples of fixed sizes. TorchGeo has many samplers, but here we will use the `RandomGeoSampler` class. Basically, the sampler selects random bounding boxes of fixed size that belongs to the original image. Then, these bounding boxes are used in the `RasterDataset` to query the portion of the image we want. Here is an example using the previously created samplers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8VGVIWNPI_W"
      },
      "outputs": [],
      "source": [
        "bbox = next(iter(train_sampler))\n",
        "bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFCssvlqPI87"
      },
      "outputs": [],
      "source": [
        "sample = train_dset[bbox]\n",
        "sample.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-gF-8k1PI6N"
      },
      "outputs": [],
      "source": [
        "sample['image'].shape, sample['mask'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGiqU2TcPcTq"
      },
      "source": [
        "Notice we have now patches of same size (..., 512 x 512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBNfy4X5Pn-G"
      },
      "source": [
        "## Creating Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a97dvCYVP5D5"
      },
      "source": [
        "Creating a `DataLoader` in TorchGeo is very straightforward, just like it is with Pytorch (we are actually using the same class). Note below that we are also using the same samplers already defined. Additionally we inform the dataset that the dataloader will use to pull data from, the batch_size (number of samples in each batch) and a collate function that specifies how to “concatenate” the multiple samples into one single batch.\n",
        "\n",
        "Finally, we can iterate through the dataloader to grab batches from it. To test it, we will get the first batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhWa0SYfav2V"
      },
      "outputs": [],
      "source": [
        "# Adjust the batch size according to your GPU memory\n",
        "train_dataloader = DataLoader(\n",
        "    train_dset, sampler=train_sampler, batch_size=4, collate_fn=stack_samples\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dset, sampler=valid_sampler, batch_size=4, collate_fn=stack_samples\n",
        ")\n",
        "\n",
        "train_batch = next(iter(train_dataloader))\n",
        "valid_batch = next(iter(valid_dataloader))\n",
        "train_batch.keys(), valid_batch.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7VRAzpkQIvr"
      },
      "source": [
        "## Batch Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78IKqlWUQMTM"
      },
      "source": [
        "Now that we can draw batches from our datasets, let’s create a function to display the batches.\n",
        "\n",
        "The function `plot_batch` will will check automatically the number of items in the batch and if there are masks associated to arrange the output grid accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zynH3etxQQmG"
      },
      "outputs": [],
      "source": [
        "def plot_imgs(\n",
        "    images: Iterable, axs: Iterable, chnls: list[int] = [2, 1, 0], bright: float = 3.0\n",
        "):\n",
        "    for img, ax in zip(images, axs):\n",
        "        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n",
        "        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n",
        "        ax.imshow(rgb)\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_msks(masks: Iterable, axs: Iterable):\n",
        "    for mask, ax in zip(masks, axs):\n",
        "        ax.imshow(mask.squeeze().numpy(), cmap='Blues')\n",
        "        ax.axis('off')\n",
        "\n",
        "\n",
        "def plot_batch(\n",
        "    batch: dict,\n",
        "    bright: float = 3.0,\n",
        "    cols: int = 4,\n",
        "    width: int = 5,\n",
        "    chnls: list[int] = [2, 1, 0],\n",
        "):\n",
        "    # Get the samples and the number of items in the batch\n",
        "    samples = unbind_samples(batch.copy())\n",
        "\n",
        "    # if batch contains images and masks, the number of images will be doubled\n",
        "    n = 2 * len(samples) if ('image' in batch) and ('mask' in batch) else len(samples)\n",
        "\n",
        "    # calculate the number of rows in the grid\n",
        "    rows = n // cols + (1 if n % cols != 0 else 0)\n",
        "\n",
        "    # create a grid\n",
        "    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n",
        "\n",
        "    if ('image' in batch) and ('mask' in batch):\n",
        "        # plot the images on the even axis\n",
        "        plot_imgs(\n",
        "            images=map(lambda x: x['image'], samples),\n",
        "            axs=axs.reshape(-1)[::2],\n",
        "            chnls=chnls,\n",
        "            bright=bright,\n",
        "        )\n",
        "\n",
        "        # plot the masks on the odd axis\n",
        "        plot_msks(masks=map(lambda x: x['mask'], samples), axs=axs.reshape(-1)[1::2])\n",
        "\n",
        "    else:\n",
        "        if 'image' in batch:\n",
        "            plot_imgs(\n",
        "                images=map(lambda x: x['image'], samples),\n",
        "                axs=axs.reshape(-1),\n",
        "                chnls=chnls,\n",
        "                bright=bright,\n",
        "            )\n",
        "\n",
        "        elif 'mask' in batch:\n",
        "            plot_msks(masks=map(lambda x: x['mask'], samples), axs=axs.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S4DZa8aQd8Z"
      },
      "outputs": [],
      "source": [
        "plot_batch(train_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGQoaWlQVFC"
      },
      "source": [
        "## Data Standardization and Spectral Indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvce_wKMQ3pT"
      },
      "source": [
        "Normally, machine learning methods (deep learning included) benefit from feature scaling. That means standard deviation around 1 and zero mean, by applying the following formula: <br>\n",
        "$X'=\\frac{X-Mean}{\\text{Standard deviation}}$\n",
        "\n",
        "To do that, we need to first find the mean and standard deviation for each one of the 6s channels in the dataset.\n",
        "\n",
        "Let’s define a function calculate these statistics and write its results in the variables mean and std. We will use our previously installed rasterio package to open the images and perform a simple average over the statistics for each batch/channel. For the standard deviation, this method is an approximation. For a more precise calculation, please refer to: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.htm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ5yXZPzNIdh"
      },
      "outputs": [],
      "source": [
        "def calc_statistics(dset: RasterDataset):\n",
        "    \"\"\"\n",
        "    Calculate the statistics (mean and std) for the entire dataset\n",
        "    Warning: This is an approximation. The correct value should take into account the\n",
        "    mean for the whole dataset for computing individual stds.\n",
        "    For correctness I suggest checking: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
        "    \"\"\"\n",
        "\n",
        "    # To avoid loading the entire dataset in memory, we will loop through each img\n",
        "    # The filenames will be retrieved from the dataset's rtree index\n",
        "    files = [\n",
        "        item.object for item in dset.index.intersection(dset.index.bounds, objects=True)\n",
        "    ]\n",
        "\n",
        "    # Resetting statistics\n",
        "    accum_mean = 0\n",
        "    accum_std = 0\n",
        "\n",
        "    for file in files:\n",
        "        img = rio.open(file).read() / 10000  # type: ignore\n",
        "        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n",
        "        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n",
        "\n",
        "    # at the end, we shall have 2 vectors with length n=chnls\n",
        "    # we will average them considering the number of images\n",
        "    return accum_mean / len(files), accum_std / len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VtubMAPxXYq"
      },
      "outputs": [],
      "source": [
        "# Calculate the statistics (Mean and std) for the dataset\n",
        "mean, std = calc_statistics(train_imgs)\n",
        "\n",
        "# Please, note that we will create spectral indices using the raw (non-normalized) data. Then, when normalizing, the sensors will have more channels (the indices) that should not be normalized.\n",
        "# To solve this, we will add the indices to the 0's to the mean vector and 1's to the std vectors\n",
        "mean = np.concat([mean, [0, 0, 0]])\n",
        "std = np.concat([std, [1, 1, 1]])\n",
        "\n",
        "norm = K.Normalize(mean=mean, std=std)\n",
        "\n",
        "tfms = torch.nn.Sequential(\n",
        "    indices.AppendNDWI(index_green=1, index_nir=3),\n",
        "    indices.AppendNDWI(index_green=1, index_nir=5),\n",
        "    indices.AppendNDVI(index_nir=3, index_red=2),\n",
        "    norm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVsDAOaVRhiN"
      },
      "outputs": [],
      "source": [
        "transformed_img = tfms(train_batch['image'])\n",
        "print(transformed_img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA-jTEKeRuA4"
      },
      "source": [
        "Note that our transformed batch has now 9 channels, instead of 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWfUOS1RR14g"
      },
      "source": [
        "> Important: the normalize method we created will apply the normalization just to the original bands and it will ignore the previously appended indices. That’s important to avoid errors due to distinct shapes between the batch and the mean and std vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfNmPcwWSNFv"
      },
      "source": [
        "## Segmentation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmXdpZWSSNR"
      },
      "source": [
        "For the semantic segmentation model, we are going to use a predefined architecture that is available in Pytorch. Looking at list (https://pytorch.org/vision/stable/models.html#semantic-segmentation) it is possible to note 3 models available for semantic segmentation, but one (LRASPP) is intended for mobile applications. In our tutorial, we will use the DeepLabV3 model.\n",
        "\n",
        "Here, we will create a DeepLabV3 model for 2 classes. In this case, I will skip the pretrained weights, as the weights represent another domain (not water segmentation from multispectral imagery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzhOtV3IyubJ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "\n",
        "model = deeplabv3_resnet50(weights=None, num_classes=2)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvsK_LXuSmnL"
      },
      "source": [
        "The first thing we have to pay attention in the model architecture is the number of channels expected in the first convolution (Conv2d), that is defined as 3. That’s because the model is prepared to work with RGB images. After the first convolution, the 3 channels will produce 64 channels in lower resolution, and so on. As we have now 9 channels, we will change this first processing layer to adapt correctly to our model. We can do this by replacing the first convolutional layer for a new one, by following the commands. Finally, we check a mock batch can pass through the model and provide the output with 2 channels (water / no_water) as desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYPdGBPOSfHJ"
      },
      "outputs": [],
      "source": [
        "backbone = model.get_submodule('backbone')\n",
        "\n",
        "conv = torch.nn.modules.conv.Conv2d(\n",
        "    in_channels=9,\n",
        "    out_channels=64,\n",
        "    kernel_size=(7, 7),\n",
        "    stride=(2, 2),\n",
        "    padding=(3, 3),\n",
        "    bias=False,\n",
        ")\n",
        "backbone.register_module('conv1', conv)\n",
        "\n",
        "pred = model(torch.randn(3, 9, 512, 512))\n",
        "pred['out'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvvGWtMrTdCE"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuUPhlzxTftk"
      },
      "source": [
        "The training function should receive the number of epochs, the model, the dataloaders, the loss function (to be optimized) the accuracy function (to assess the results), the optimizer (that will adjust the parameters of the model in the correct direction) and the transformations to be applied to each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EJF-loJN6iF"
      },
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFlsT8glSq3x"
      },
      "outputs": [],
      "source": [
        "def train_loop(\n",
        "    epochs: int,\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader | None,\n",
        "    model: torch.nn.Module,\n",
        "    loss_fn: Callable,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    acc_fns: list | None = None,\n",
        "    batch_tfms: Callable | None = None,\n",
        "):\n",
        "    # size = len(dataloader.dataset)\n",
        "    cuda_model = model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        accum_loss = 0\n",
        "        for batch in train_dl:\n",
        "            if batch_tfms is not None:\n",
        "                X = batch_tfms(batch['image']).to(device)\n",
        "            else:\n",
        "                X = batch['image'].to(device)\n",
        "\n",
        "            y = batch['mask'].type(torch.long).to(device)\n",
        "            pred = cuda_model(X)['out']\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            # BackProp\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update the accum loss\n",
        "            accum_loss += float(loss) / len(train_dl)\n",
        "\n",
        "        # Testing against the validation dataset\n",
        "        if acc_fns is not None and val_dl is not None:\n",
        "            # reset the accuracies metrics\n",
        "            acc = [0.0] * len(acc_fns)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dl:\n",
        "                    if batch_tfms is not None:\n",
        "                        X = batch_tfms(batch['image']).to(device)\n",
        "                    else:\n",
        "                        X = batch['image'].type(torch.float32).to(device)\n",
        "\n",
        "                    y = batch['mask'].type(torch.long).to(device)\n",
        "\n",
        "                    pred = cuda_model(X)['out']\n",
        "\n",
        "                    for i, acc_fn in enumerate(acc_fns):\n",
        "                        acc[i] = float(acc[i] + acc_fn(pred, y) / len(val_dl))\n",
        "\n",
        "            # at the end of the epoch, print the errors, etc.\n",
        "            print(\n",
        "                f'Epoch {epoch}: Train Loss={accum_loss:.5f} - Accs={[round(a, 3) for a in acc]}'\n",
        "            )\n",
        "        else:\n",
        "            print(f'Epoch {epoch}: Train Loss={accum_loss:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUIPlgndUB_9"
      },
      "source": [
        "## Loss and Accuracy Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs5LOs8LUS7j"
      },
      "source": [
        "For the loss function, normally the Cross Entropy Loss should work, but it requires the mask to have shape (N, d1, d2). In this case, we will need to squeeze our second dimension manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8K3e5NwTrYg"
      },
      "outputs": [],
      "source": [
        "def oa(pred, y):\n",
        "    flat_y = y.squeeze()\n",
        "    flat_pred = pred.argmax(dim=1)\n",
        "    acc = torch.count_nonzero(flat_y == flat_pred) / torch.numel(flat_y)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def iou(pred, y):\n",
        "    flat_y = y.cpu().numpy().squeeze()\n",
        "    flat_pred = pred.argmax(dim=1).detach().cpu().numpy()\n",
        "    return jaccard_score(flat_y.reshape(-1), flat_pred.reshape(-1), zero_division=1.0)\n",
        "\n",
        "\n",
        "def loss(p, t):\n",
        "    return torch.nn.functional.cross_entropy(p, t.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW6YfmnyUa1A"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xajn1unSUh3h"
      },
      "source": [
        "> To train the model it is important to have CUDA GPUs available. In Colab, it can be done by changing the runtime type and re-running the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moU9ol78UUqP"
      },
      "outputs": [],
      "source": [
        "# adjust number of epochs depending on the device\n",
        "if torch.cuda.is_available():\n",
        "    num_epochs = 2\n",
        "else:\n",
        "    # if GPU is not available, just make 1 pass and limit the size of the datasets\n",
        "    num_epochs = 1\n",
        "\n",
        "    # by limiting the length of the sampler we limit the iterations in each epoch\n",
        "    train_dataloader.sampler.length = 8\n",
        "    valid_dataloader.sampler.length = 8\n",
        "\n",
        "# train the model\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "train_loop(\n",
        "    num_epochs,\n",
        "    train_dataloader,\n",
        "    valid_dataloader,\n",
        "    model,\n",
        "    loss,\n",
        "    optimizer,\n",
        "    acc_fns=[oa, iou],\n",
        "    batch_tfms=tfms,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE3qXpiMN6iF"
      },
      "source": [
        "## Additional Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbMR6P9CN6iF"
      },
      "source": [
        "This tutorial is also available as a 3 parts Medium story: <br>https://medium.com/towards-data-science/artificial-intelligence-for-geospatial-analysis-with-pytorchs-torchgeo-part-1-52d17e409f09"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}